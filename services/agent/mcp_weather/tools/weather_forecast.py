import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List
import urllib3

# Disable SSL warnings for government sites
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Bengali to English district name mapping
BENGALI_TO_ENGLISH_DISTRICTS = {
    "à¦ªà¦¾à¦¬à¦¨à¦¾": "Pabna", "à¦¢à¦¾à¦•à¦¾": "Dhaka", "à¦šà¦Ÿà§à¦Ÿà¦—à§à¦°à¦¾à¦®": "Chattogram",
    "à¦•à¦•à§à¦¸à¦¬à¦¾à¦œà¦¾à¦°": "Cox's Bazar", "à¦¸à¦¿à¦²à§‡à¦Ÿ": "Sylhet", "à¦°à¦¾à¦œà¦¶à¦¾à¦¹à§€": "Rajshahi",
    "à¦–à§à¦²à¦¨à¦¾": "Khulna", "à¦°à¦‚à¦ªà§à¦°": "Rangpur", "à¦¬à¦°à¦¿à¦¶à¦¾à¦²": "Barishal",
    "à¦®à¦¯à¦¼à¦®à¦¨à¦¸à¦¿à¦‚à¦¹": "Mymensingh", "à¦•à§à¦®à¦¿à¦²à§à¦²à¦¾": "Cumilla", "à¦¦à¦¿à¦¨à¦¾à¦œà¦ªà§à¦°": "Dinajpur",
    "à¦¯à¦¶à§‹à¦°": "Jashore", "à¦¬à¦—à§à¦¡à¦¼à¦¾": "Bogura", "à¦¨à§‹à¦¯à¦¼à¦¾à¦–à¦¾à¦²à§€": "Noakhali",
    "à¦«à§‡à¦¨à§€": "Feni", "à¦œà¦¾à¦®à¦¾à¦²à¦ªà§à¦°": "Jamalpur", "à¦•à¦¿à¦¶à§‹à¦°à¦—à¦žà§à¦œ": "Kishoreganj",
    "à¦®à¦¾à¦¨à¦¿à¦•à¦—à¦žà§à¦œ": "Manikganj", "à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ": "Munshiganj", "à¦«à¦°à¦¿à¦¦à¦ªà§à¦°": "Faridpur",
    "à¦—à§‹à¦ªà¦¾à¦²à¦—à¦žà§à¦œ": "Gopalganj", "à¦®à¦¾à¦¦à¦¾à¦°à§€à¦ªà§à¦°": "Madaripur", "à¦°à¦¾à¦œà¦¬à¦¾à¦¡à¦¼à§€": "Rajbari",
    "à¦¶à¦°à§€à¦¯à¦¼à¦¤à¦ªà§à¦°": "Shariatpur", "à¦ªà¦Ÿà§à¦¯à¦¼à¦¾à¦–à¦¾à¦²à§€": "Patuakhali", "à¦ªà¦¿à¦°à§‹à¦œà¦ªà§à¦°": "Pirojpur",
    "à¦¬à¦°à¦—à§à¦¨à¦¾": "Barguna", "à¦­à§‹à¦²à¦¾": "Bhola", "à¦à¦¾à¦²à¦•à¦¾à¦ à¦¿": "Jhalokati",
    "à¦²à¦•à§à¦·à§à¦®à§€à¦ªà§à¦°": "Lakshmipur", "à¦¸à¦¾à¦¤à¦•à§à¦·à§€à¦°à¦¾": "Satkhira", "à¦¬à¦¾à¦—à§‡à¦°à¦¹à¦¾à¦Ÿ": "Bagerhat",
    "à¦¬à¦¾à¦¨à§à¦¦à¦°à¦¬à¦¾à¦¨": "Bandarban", "à¦°à¦¾à¦™à§à¦—à¦¾à¦®à¦¾à¦Ÿà¦¿": "Rangamati", "à¦–à¦¾à¦—à¦¡à¦¼à¦¾à¦›à¦¡à¦¼à¦¿": "Khagrachari",
    "à¦ à¦¾à¦•à§à¦°à¦—à¦¾à¦à¦“": "Thakurgaon", "à¦ªà¦žà§à¦šà¦—à¦¡à¦¼": "Panchagarh", "à¦²à¦¾à¦²à¦®à¦¨à¦¿à¦°à¦¹à¦¾à¦Ÿ": "Lalmonirhat",
    "à¦•à§à¦¡à¦¼à¦¿à¦—à§à¦°à¦¾à¦®": "Kurigram", "à¦—à¦¾à¦‡à¦¬à¦¾à¦¨à§à¦§à¦¾": "Gaibandha", "à¦¨à§€à¦²à¦«à¦¾à¦®à¦¾à¦°à§€": "Nilphamari",
    "à¦šà¦¾à¦à¦ªà¦¾à¦‡à¦¨à¦¬à¦¾à¦¬à¦—à¦žà§à¦œ": "Chapai Nawabganj", "à¦¨à¦“à¦—à¦¾à¦": "Naogaon", "à¦¨à¦¡à¦¼à¦¾à¦‡à¦²": "Narail",
    "à¦¨à¦¾à¦°à¦¾à¦¯à¦¼à¦£à¦—à¦žà§à¦œ": "Narayanganj", "à¦¨à¦°à¦¸à¦¿à¦‚à¦¦à§€": "Narsingdi", "à¦¨à¦¾à¦Ÿà§‹à¦°": "Natore",
    "à¦¨à§‡à¦¤à§à¦°à¦•à§‹à¦¨à¦¾": "Netrokona", "à¦¹à¦¬à¦¿à¦—à¦žà§à¦œ": "Habiganj", "à¦®à§Œà¦²à¦­à§€à¦¬à¦¾à¦œà¦¾à¦°": "Moulvibazar",
    "à¦¸à§à¦¨à¦¾à¦®à¦—à¦žà§à¦œ": "Sunamganj", "à¦Ÿà¦¾à¦™à§à¦—à¦¾à¦‡à¦²": "Tangail", "à¦—à¦¾à¦œà§€à¦ªà§à¦°": "Gazipur",
    "à¦šà¦¾à¦à¦¦à¦ªà§à¦°": "Chandpur", "à¦¬à§à¦°à¦¾à¦¹à§à¦®à¦£à¦¬à¦¾à¦¡à¦¼à¦¿à¦¯à¦¼à¦¾": "Brahmanbaria", "à¦šà§à¦¯à¦¼à¦¾à¦¡à¦¾à¦™à§à¦—à¦¾": "Chuadanga",
    "à¦à¦¿à¦¨à¦¾à¦‡à¦¦à¦¹": "Jhenaidah", "à¦œà¦¯à¦¼à¦ªà§à¦°à¦¹à¦¾à¦Ÿ": "Joypurhat", "à¦•à§à¦·à§à¦Ÿà¦¿à¦¯à¦¼à¦¾": "Kushtia",
    "à¦®à¦¾à¦—à§à¦°à¦¾": "Magura", "à¦®à§‡à¦¹à§‡à¦°à¦ªà§à¦°": "Meherpur", "à¦¶à§‡à¦°à¦ªà§à¦°": "Sherpur",
    "à¦¸à¦¿à¦°à¦¾à¦œà¦—à¦žà§à¦œ": "Sirajganj"
}

class BMDWeatherScraper:
    @staticmethod
    def _normalize_district_name(name: str) -> str:
        """Normalize district name for matching (remove spaces, special chars)"""
        return re.sub(r'[\s\'\-]', '', name.lower())

    @staticmethod
    def scrape_forecast(days: int, target_district: str) -> dict:
        url = f"https://www.bamis.gov.bd/en/bmd/wrf/table/all/{days}/"
        print(f"Scraping BMD WRF {days}-day forecast from: {url}")
        
        # 1. Handle Bengali to English conversion
        search_name = target_district.lower().strip()
        if any('\u0980' <= c <= '\u09FF' for c in search_name):
            for bengali, english in BENGALI_TO_ENGLISH_DISTRICTS.items():
                if bengali in search_name or search_name in bengali:
                    search_name = english.lower()
                    print(f"ðŸ”„ Translated {target_district} to {english}")
                    break

        try:
            print(f"âœ… Input district '{target_district}' assumed to be English, no translation used.")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            resp = requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            table = soup.find('table')
            if not table: return None
            
            rows = table.find_all('tr')[1:] 
            norm_search = BMDWeatherScraper._normalize_district_name(search_name)
            print(f"DEBUG: Searching for district '{search_name}' -> normalized '{norm_search}'")

            for row in rows:
                cols = row.find_all(['td', 'th'])
                if not cols or len(cols) < 12: continue
                
                db_district = cols[0].get_text(strip=True).lower()
                norm_db = BMDWeatherScraper._normalize_district_name(db_district)
                print(f"DEBUG: Table row district='{db_district}' -> normalized='{norm_db}'")

                if norm_search in norm_db or norm_db in norm_search:
                    print(f"âœ… Match found: {db_district}")
                    try:
                        # BAMIS Indices: 1:MinT, 3:MaxT, 5:Hum, 10:Rain
                        t_min = float(re.search(r"[\d\.]+", cols[1].text).group())
                        t_max = float(re.search(r"[\d\.]+", cols[3].text).group())
                        hum   = float(re.search(r"[\d\.]+", cols[5].text).group())
                        rain  = float(re.search(r"[\d\.]+", cols[10].text).group())
                    except (ValueError, AttributeError, IndexError):
                        continue

                    daily_rain = round(rain / days, 1) if days > 0 else 0
                    forecast = []
                    for i in range(days):
                        forecast.append({
                            "date": (datetime.now() + timedelta(days=i+1)).strftime("%Y-%m-%d"),
                            "parameters": {
                                "temperature": {"min": t_min, "max": t_max, "unit": "Celsius"},
                                "precipitation": {"value": daily_rain, "unit": "mm", "probability": min(daily_rain / 10, 1) },
                                "humidity": {"value": hum, "unit": "percent"}
                            }
                        })
                    
                    return {
                        "location": {"area_name": db_district.title()},
                        "forecast": forecast
                    }
            return None
        except Exception as e:
            print(f"âŒ Scraper error: {e}")
            return None

def retrieve_weather_forecast(district: str, forecast_days: int, parameters: List[str] = None) -> str:
    data = BMDWeatherScraper.scrape_forecast(forecast_days, district)
    
    if not data:
        return json.dumps({
            "error": f"District '{district}' not found in BAMIS table.",
            "available_districts_hint": "Try English or Bengali script (à¦ªà¦¾à¦¬à¦¨à¦¾)."
        })
    
    return json.dumps(data)